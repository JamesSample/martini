{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import nivapy3 as nivapy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patching missing data in Martini river inputs\n",
    "\n",
    "[Notebook 1](https://nbviewer.jupyter.org/github/JamesSample/martini/blob/master/notebooks/01_data_pre-processing.ipynb) and [notebook 2](https://nbviewer.jupyter.org/github/JamesSample/martini/blob/master/notebooks/02_riverine_inputs.ipynb) generated daily time series of river concentrations based on available monitoring data. Results are summaries in the Excel file [here](https://github.com/JamesSample/martini/blob/master/data/tidy/martini_daily_conc_q_summary.xlsx).\n",
    "\n",
    "There are significant gaps in the dataset for some sites and variables. This notebook attempts to patch missing data using national aggregated values, where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>andre_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>country</th>\n",
       "      <th>missing_vars</th>\n",
       "      <th>status</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dmca003</td>\n",
       "      <td>Vigsø bukt</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>All</td>\n",
       "      <td>No data available</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>oda_4000005</td>\n",
       "      <td>dmca004</td>\n",
       "      <td>West side of Nord-Jylland - Liver Å</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>TOC, DOC, SiO2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>oda_3000002</td>\n",
       "      <td>dmca005</td>\n",
       "      <td>Hirtshals-Skagen - Uggerby Å</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>TOC, DOC, SiO2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dmca006</td>\n",
       "      <td>Skagen- Lyngsaa strand</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>All</td>\n",
       "      <td>No data available</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dmca007</td>\n",
       "      <td>East side of Nord-Jylland</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>All</td>\n",
       "      <td>No data available</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    station_id andre_id                         station_name  country  \\\n",
       "0          NaN  dmca003                           Vigsø bukt  Denmark   \n",
       "1  oda_4000005  dmca004  West side of Nord-Jylland - Liver Å  Denmark   \n",
       "2  oda_3000002  dmca005         Hirtshals-Skagen - Uggerby Å  Denmark   \n",
       "3          NaN  dmca006               Skagen- Lyngsaa strand  Denmark   \n",
       "4          NaN  dmca007            East side of Nord-Jylland  Denmark   \n",
       "\n",
       "     missing_vars             status comment  \n",
       "0             All  No data available     NaN  \n",
       "1  TOC, DOC, SiO2                NaN     NaN  \n",
       "2  TOC, DOC, SiO2                NaN     NaN  \n",
       "3             All  No data available     NaN  \n",
       "4             All  No data available     NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read station metadata\n",
    "xl_path = r'../data/tidy/martini_daily_conc_q_summary.xlsx'\n",
    "stn_df = pd.read_excel(xl_path, sheet_name='stations')\n",
    "stn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read conc. data\n",
    "xl_path = r'../data/tidy/martini_daily_conc_q_summary.xlsx'\n",
    "df = pd.read_excel(xl_path, sheet_name='daily_q_concs')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SRP and TIP columns to TIP\n",
    "# Denmark reports SRP, others report TIP. We're assuming these are the same\n",
    "df['TIP_µg/l'] = df['TIP_µg/l'].where(df['TIP_µg/l'].notnull(), df['SRP_µg/l'])\n",
    "del df['SRP_µg/l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt to long format for later\n",
    "df_long = df.melt(id_vars=['station_id', 'sample_date'])\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing national median time series\n",
    "patch_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Denmark\n",
    "\n",
    "All Danish stations are missing TOC, DOC and SiO2. There are 9 stations in Denmark with complete datasets for the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danish stations with data\n",
    "den_stn_df = stn_df.query('country == \"Denmark\"').dropna(subset=['station_id'])\n",
    "den_stn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chem for Danish stations\n",
    "stn_list = list(den_stn_df['station_id'].unique())\n",
    "den_df = df.query('station_id in @stn_list').copy()\n",
    "\n",
    "# Drop flow, TOC, DOC and SiO2\n",
    "den_df.drop(['flow_m3/s', 'DOC_mg/l', 'TOC_mg/l', 'SIO2_µg/l'], \n",
    "            inplace=True, \n",
    "            axis='columns')\n",
    "\n",
    "den_df = den_df.melt(id_vars=['station_id', 'sample_date'])\n",
    "\n",
    "den_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot range of values for each variable\n",
    "fig, axes = plt.subplots(nrows=len(den_df['variable'].unique()), ncols=1, figsize=(12,40))\n",
    "\n",
    "# Loop over Danish data\n",
    "for idx, var_name in enumerate(den_df['variable'].unique()):\n",
    "    # Get data\n",
    "    var_df = den_df.query('variable == @var_name')\n",
    "    \n",
    "    # Loop over stations\n",
    "    df_list = []\n",
    "    for stn_id in var_df['station_id'].unique():\n",
    "        # Get data for stn\n",
    "        stn_var_df = var_df.query('station_id == @stn_id')\n",
    "        stn_var_df = stn_var_df[['sample_date', 'value']]\n",
    "        stn_var_df.set_index('sample_date', inplace=True)\n",
    "        df_list.append(stn_var_df)\n",
    "        stn_var_df.plot(ax=axes[idx], legend=False)\n",
    "    \n",
    "    # Get median over all stations for this variable\n",
    "    agg_df = pd.concat(df_list, axis='columns')\n",
    "    med_df = agg_df.median(axis='columns')\n",
    "    patch_dict[('Denmark', var_name)] = med_df\n",
    "    \n",
    "    # Plot\n",
    "    med_df.plot(ax=axes[idx], legend=False, lw=3, c='k')\n",
    "    axes[idx].set_title(var_name)\n",
    "    axes[idx].set_xlabel('')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consitency, adjust values for TON, N-TOT, TOP and P-TOT so that totals make sense\n",
    "patch_dict[('Denmark', 'TON_µg/l')] = (patch_dict[('Denmark', 'DON_µg/l')] + \n",
    "                                       patch_dict[('Denmark', 'PON_µg/l')]) \n",
    "\n",
    "patch_dict[('Denmark', 'N-TOT_µg/l')] = (patch_dict[('Denmark', 'TON_µg/l')] + \n",
    "                                         patch_dict[('Denmark', 'N-NH4_µg/l')] + \n",
    "                                         patch_dict[('Denmark', 'N-SNOX_µg/l')])\n",
    "\n",
    "patch_dict[('Denmark', 'TOP_µg/l')] = (patch_dict[('Denmark', 'DOP_µg/l')] + \n",
    "                                       patch_dict[('Denmark', 'POP_µg/l')])\n",
    "\n",
    "patch_dict[('Denmark', 'P-TOT_µg/l')] = (patch_dict[('Denmark', 'TOP_µg/l')] + \n",
    "                                         patch_dict[('Denmark', 'TIP_µg/l')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Norway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norway stations with complete data\n",
    "nor_stn_df = stn_df.query('country == \"Norway\"').query('status == \"OK\"')\n",
    "nor_stn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chem for Norwegian stations\n",
    "stn_list = list(nor_stn_df['station_id'].unique())\n",
    "nor_df = df.query('station_id in @stn_list').copy()\n",
    "\n",
    "# Drop flow\n",
    "nor_df.drop(['flow_m3/s'], \n",
    "            inplace=True, \n",
    "            axis='columns')\n",
    "\n",
    "nor_df = nor_df.melt(id_vars=['station_id', 'sample_date'])\n",
    "\n",
    "nor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot range of values for each variable\n",
    "fig, axes = plt.subplots(nrows=len(nor_df['variable'].unique()), ncols=1, figsize=(12,40))\n",
    "\n",
    "# Loop over Danish data\n",
    "for idx, var_name in enumerate(nor_df['variable'].unique()):\n",
    "    # Get data\n",
    "    var_df = nor_df.query('variable == @var_name')\n",
    "    \n",
    "    # Loop over stations\n",
    "    df_list = []\n",
    "    for stn_id in var_df['station_id'].unique():\n",
    "        # Get data for stn\n",
    "        stn_var_df = var_df.query('station_id == @stn_id')\n",
    "        stn_var_df = stn_var_df[['sample_date', 'value']]\n",
    "        stn_var_df.set_index('sample_date', inplace=True)\n",
    "        df_list.append(stn_var_df)\n",
    "        stn_var_df.plot(ax=axes[idx], legend=False)\n",
    "    \n",
    "    # Get median over all stations for this variable\n",
    "    agg_df = pd.concat(df_list, axis='columns')\n",
    "    med_df = agg_df.median(axis='columns')\n",
    "    patch_dict[('Norway', var_name)] = med_df\n",
    "    \n",
    "    # Plot\n",
    "    med_df.plot(ax=axes[idx], legend=False, lw=3, c='k')\n",
    "    axes[idx].set_title(var_name)\n",
    "    axes[idx].set_xlabel('')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consitency, adjust values for TON, N-TOT, TOP and P-TOT so that totals make sense\n",
    "patch_dict[('Norway', 'TON_µg/l')] = (patch_dict[('Norway', 'DON_µg/l')] + \n",
    "                                       patch_dict[('Norway', 'PON_µg/l')]) \n",
    "\n",
    "patch_dict[('Norway', 'N-TOT_µg/l')] = (patch_dict[('Norway', 'TON_µg/l')] + \n",
    "                                         patch_dict[('Norway', 'N-NH4_µg/l')] + \n",
    "                                         patch_dict[('Norway', 'N-SNOX_µg/l')])\n",
    "\n",
    "patch_dict[('Norway', 'TOP_µg/l')] = (patch_dict[('Norway', 'DOP_µg/l')] + \n",
    "                                       patch_dict[('Norway', 'POP_µg/l')])\n",
    "\n",
    "patch_dict[('Norway', 'P-TOT_µg/l')] = (patch_dict[('Norway', 'TOP_µg/l')] + \n",
    "                                         patch_dict[('Norway', 'TIP_µg/l')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweden stations with near-complete data\n",
    "swe_stn_df = stn_df.query('country == \"Sweden\"').query('status == \"OK\"')\n",
    "swe_stn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chem for Swedish stations\n",
    "stn_list = list(swe_stn_df['station_id'].unique())\n",
    "swe_df = df.query('station_id in @stn_list').copy()\n",
    "\n",
    "# Drop flow and STS\n",
    "swe_df.drop(['flow_m3/s', 'STS_mg/l'], \n",
    "            inplace=True, \n",
    "            axis='columns')\n",
    "\n",
    "swe_df = swe_df.melt(id_vars=['station_id', 'sample_date'])\n",
    "\n",
    "swe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot range of values for each variable\n",
    "fig, axes = plt.subplots(nrows=len(swe_df['variable'].unique()), ncols=1, figsize=(12,40))\n",
    "\n",
    "# Loop over Danish data\n",
    "for idx, var_name in enumerate(swe_df['variable'].unique()):\n",
    "    # Get data\n",
    "    var_df = swe_df.query('variable == @var_name')\n",
    "    \n",
    "    # Loop over stations\n",
    "    df_list = []\n",
    "    for stn_id in var_df['station_id'].unique():\n",
    "        # Get data for stn\n",
    "        stn_var_df = var_df.query('station_id == @stn_id')\n",
    "        stn_var_df = stn_var_df[['sample_date', 'value']]\n",
    "        stn_var_df.set_index('sample_date', inplace=True)\n",
    "        df_list.append(stn_var_df)\n",
    "        stn_var_df.plot(ax=axes[idx], legend=False)\n",
    "    \n",
    "    # Get median over all stations for this variable\n",
    "    agg_df = pd.concat(df_list, axis='columns')\n",
    "    med_df = agg_df.median(axis='columns')\n",
    "    patch_dict[('Sweden', var_name)] = med_df\n",
    "    \n",
    "    # Plot\n",
    "    med_df.plot(ax=axes[idx], legend=False, lw=3, c='k')\n",
    "    axes[idx].set_title(var_name)\n",
    "    axes[idx].set_xlabel('')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consitency, adjust values for TON, N-TOT, TOP and P-TOT so that totals make sense\n",
    "patch_dict[('Sweden', 'TON_µg/l')] = (patch_dict[('Sweden', 'DON_µg/l')] + \n",
    "                                       patch_dict[('Sweden', 'PON_µg/l')]) \n",
    "\n",
    "patch_dict[('Sweden', 'N-TOT_µg/l')] = (patch_dict[('Sweden', 'TON_µg/l')] + \n",
    "                                         patch_dict[('Sweden', 'N-NH4_µg/l')] + \n",
    "                                         patch_dict[('Sweden', 'N-SNOX_µg/l')])\n",
    "\n",
    "patch_dict[('Sweden', 'TOP_µg/l')] = (patch_dict[('Sweden', 'DOP_µg/l')] + \n",
    "                                       patch_dict[('Sweden', 'POP_µg/l')])\n",
    "\n",
    "patch_dict[('Sweden', 'P-TOT_µg/l')] = (patch_dict[('Sweden', 'TOP_µg/l')] + \n",
    "                                         patch_dict[('Sweden', 'TIP_µg/l')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values as medians from entire dataset where pathing at national level not possible\n",
    "def_val_dict = {('Sweden', 'STS_mg/l')  : df['STS_mg/l'].median().round(0),\n",
    "                ('Denmark', 'TOC_mg/l') : df['TOC_mg/l'].median().round(0),\n",
    "                ('Denmark', 'DOC_mg/l') : df['DOC_mg/l'].median().round(0),\n",
    "                ('Denmark', 'SIO2_µg/l'): df['SIO2_µg/l'].median().round(0),\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of Martini outflow locations\n",
    "mar_stns = stn_df.dropna(subset=['andre_id']).drop_duplicates(subset=['andre_id'])\n",
    "mar_stns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# All vars of interest, excluding flow\n",
    "all_vars = [i for i in df_long['variable'].unique() if i != 'flow_m3/s']\n",
    "\n",
    "# Loop over stations of interest\n",
    "for idx, row in mar_stns.iterrows():\n",
    "    mar_id = row['andre_id']\n",
    "    stn_id = row['station_id']\n",
    "    country = row['country']\n",
    "    \n",
    "    # Loop over vars\n",
    "    for var_name in all_vars:\n",
    "        # Do we have a complete datset based on measure data?\n",
    "        stn_var_df = df_long.query('(station_id == @stn_id) and (variable == @var_name)').copy()\n",
    "        \n",
    "        if len(stn_var_df) == 0:\n",
    "            # No data at all for this station\n",
    "            stn_var_df = pd.DataFrame({'station_id': mar_id,\n",
    "                                       'sample_date':pd.date_range('2015-01-01', '2017-12-31', freq='D'),\n",
    "                                       'variable':var_name,\n",
    "                                       'value':np.nan,\n",
    "                                      })\n",
    "        assert len(stn_var_df) == 1096\n",
    "        \n",
    "        if pd.isna(stn_var_df['value']).sum() == 0:\n",
    "            # We have a complete data series\n",
    "            # Change ID to mar_id\n",
    "            stn_var_df['station_id'] = mar_id\n",
    "            df_list.append(stn_var_df)\n",
    "        else:\n",
    "            # Do we have a series based on national median values?\n",
    "            if (country, var_name) in patch_dict.keys():\n",
    "                patch_df = patch_dict[(country, var_name)].reset_index()\n",
    "                patch_df.columns = ['sample_date', 'value']\n",
    "                patch_df['variable'] = var_name\n",
    "                patch_df['station_id'] = mar_id\n",
    "                df_list.append(patch_df)\n",
    "            else:\n",
    "                # Use a constant default value\n",
    "                stn_var_df['value'] = def_val_dict[(country, var_name)]\n",
    "                \n",
    "                # Change ID to mar_id\n",
    "                stn_var_df['station_id'] = mar_id\n",
    "                df_list.append(stn_var_df)\n",
    "\n",
    "# Combine and reshape\n",
    "patched_df = pd.concat(df_list, axis='rows', sort=True)\n",
    "patched_df.set_index(['station_id', 'sample_date', 'variable'], inplace=True)\n",
    "patched_df = patched_df.unstack('variable')\n",
    "patched_df.reset_index(inplace=True)\n",
    "patched_df.columns = (list(patched_df.columns.get_level_values(0)[:2]) + \n",
    "                      list(patched_df.columns.get_level_values(1)[2:]))\n",
    "\n",
    "# Reorder cols\n",
    "cols = list(df.columns)\n",
    "cols.remove('flow_m3/s')\n",
    "patched_df = patched_df[cols]\n",
    "\n",
    "# Save\n",
    "out_csv = r'../data/tidy/martini_daily_concs_patched.csv'\n",
    "patched_df.to_csv(out_csv, index=False, encoding='utf-8')\n",
    "\n",
    "patched_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
